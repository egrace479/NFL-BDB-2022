{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60167733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helper Functions :)\n",
    "#convert feet to inches for players\n",
    "def ft_in(x):\n",
    "    if '-' in x:\n",
    "        meas=x.split('-')\n",
    "        #this will be a list ['ft','in']\n",
    "        inches = int(meas[0])*12 + int(meas[1])\n",
    "        return inches\n",
    "    else:\n",
    "        return int(x)\n",
    "    \n",
    "#convert Game Clock from MM:SS:00 to Seconds\n",
    "def clock(x,df):\n",
    "    gameClock = df.loc[x]['gameClock']\n",
    "    quarter = df.loc[x]['quarter']\n",
    "\n",
    "    gameClock_split = gameClock.split(':')\n",
    "\n",
    "    minutes = gameClock_split[0]\n",
    "    seconds = gameClock_split[1]\n",
    "\n",
    "    total_minutes = int(minutes) + 15 * (quarter - 1)\n",
    "\n",
    "    return (total_minutes * 60) + int(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39593e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_play(game_id, play_id, tracking):\n",
    "    '''\n",
    "    This function creates the tracking dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    game_id, play_id - game and play of interest\n",
    "    tracking - tracking dataframe the that game and play are in\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    play - dataframe of just the tracking data for the particular play of interest\n",
    "    '''\n",
    "    game = tracking[tracking['gameId'] == game_id]\n",
    "    play_ex = game[game['playId'] == play_id]\n",
    "    \n",
    "    return play_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfce2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocess by dataset\n",
    "#players Dataset\n",
    "def preprocess_players(players_df):\n",
    "    # preprocessing steps\n",
    "    players_df['height'] = players_df['height'].apply(ft_in)\n",
    "    return players_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d90f0",
   "metadata": {},
   "source": [
    "Note: run `preprocess_tracking` first, then `preprocess_football_track`, then `preprocess_play`.\n",
    "AFTER running these 3 functions, you can run `drop_by_index_difference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38fcfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess tracking\n",
    "def preprocess_tracking(track18, track19, track20, play_df, play_type):\n",
    "    '''\n",
    "    This function creates the tracking dataframes by play-type by year.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    track18, track19, track20 - trackYY.csv dataframes\n",
    "    play_df - play.csv dataframe\n",
    "    play_type - string, play type, e.g., 'Extra Point' \n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    track_p18 - Tracking Play Type 2018 dataframe\n",
    "    track_p19 - Tracking Play Type 2019 dataframe\n",
    "    track_p20 - Tracking Play Type 2020 dataframe\n",
    "    '''\n",
    "    track18 = track18.copy()\n",
    "    track19 = track19.copy()\n",
    "    track20 = track20.copy()\n",
    "    #re-orient direction of play by offensive team direction : \n",
    "    #We must reorient this to reflect movement in the offense direction instead of the on-field coordinates \n",
    "    #(reorient the origin from the bottom left to top right for a change in direction).\n",
    "    #2018 tracking data\n",
    "    track18.loc[track18['playDirection'] == 'left', 'x'] = 120 -track18.loc[track18['playDirection']=='left','x']\n",
    "    track18.loc[track18['playDirection'] == 'left', 'y'] = 160/3 -track18.loc[track18['playDirection']=='left','y']\n",
    "    #note that we have 160/3 for the y direction since the football field is 160ft, but our units are yards\n",
    "\n",
    "    #2019 tracking data\n",
    "    track19.loc[track19['playDirection'] == 'left', 'x'] = 120 -track19.loc[track19['playDirection']=='left','x']\n",
    "    track19.loc[track19['playDirection'] == 'left', 'y'] = 160/3 -track19.loc[track19['playDirection']=='left','y']\n",
    "\n",
    "    #2020 tracking data\n",
    "    track20.loc[track20['playDirection'] == 'left', 'x'] = 120 -track20.loc[track20['playDirection']=='left','x']\n",
    "    track20.loc[track20['playDirection'] == 'left', 'y'] = 160/3 -track20.loc[track20['playDirection']=='left','y']\n",
    "\n",
    "    #divide play dataset by type of play\n",
    "    play_p = play_df.loc[play_df['specialTeamsPlayType']== play_type][['gameId', 'playId']]\n",
    "    \n",
    "    #merge play_type with tracking for each year\n",
    "    track_p18 = pd.merge(play_p, track18, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_p19 = pd.merge(play_p, track19, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_p20 = pd.merge(play_p, track20, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    \n",
    "    return track_p18, track_p19, track_p20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d467c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_football_track(track_p18, track_p19, track_p20):\n",
    "    '''\n",
    "    This function creates the football tracking dataframe by given event.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    track_p18, track_p19, track_p20 - tracking by play by year dataframes\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    track_fp - Tracking Football Play Type dataframe\n",
    "    '''\n",
    "    \n",
    "    #separate out the football data in each year's tracking dataframe and drop null values\n",
    "    #concatenate to one dataframe for football tracking data\n",
    "    track_fp18 = track_p18.loc[track_p18['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fp19 = track_p19.loc[track_p19['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fp20 = track_p20.loc[track_p20['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fp = pd.concat([track_fp18, track_fp19, track_fp20], ignore_index = True)\n",
    "    \n",
    "    return track_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1cebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess play data\n",
    "def preprocess_play(play_df):\n",
    "    '''\n",
    "    This function fills nulls in the play dataframe and applies the clock function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - play.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    play_df - Processed play dataframe\n",
    "    '''\n",
    "    #null penalty yards = 0\n",
    "    play_df['penaltyYards']=play_df['penaltyYards'].fillna(0)\n",
    "    \n",
    "    #clock: MM:SS to Seconds\n",
    "    play_df['gameClockSeconds'] = play_df.index.map(lambda x: clock(x,play_df))\n",
    "    \n",
    "    #redefine nulls in penalty as no penalty\n",
    "    play_df['penaltyCodes']=play_df['penaltyCodes'].fillna('no penalty')\n",
    "    \n",
    "    #TO-DO: Address null values on kickerId and kickBlockerId \n",
    "    #(note their height & weight comes in too)\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95872e68",
   "metadata": {},
   "source": [
    "#### We need to make some data frames for analysis: Weather, ExtraPoint, FieldGoal, Punts, Kickoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba336d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data():\n",
    "    '''\n",
    "    This function creates the Weather dataframes by year.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    weather2018, weather2019, weather2020 - Weather dataframes by year\n",
    "\n",
    "    '''\n",
    "    # Pull down datasets\n",
    "    games = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/games.csv')\n",
    "    stadium_coordinates = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/stadium_coordinates.csv')\n",
    "    games_weather = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/games_weather.csv')\n",
    "\n",
    "    # Merge game and weather data on game_id\n",
    "    games_weather_merge = pd.merge(games_weather, games, on='game_id')\n",
    "\n",
    "    # Merge stadium data on StadiumName\n",
    "    final_df = pd.merge(games_weather_merge, stadium_coordinates, on='StadiumName')\n",
    "\n",
    "    # Convert time columns to datetime objects\n",
    "    time_cols = ['TimeMeasure', 'TimeStartGame', 'TimeEndGame']\n",
    "\n",
    "    for col in time_cols:\n",
    "        final_df[col] = pd.to_datetime(final_df[col], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "    # Create sliced DataFrames\n",
    "    weather2018 = final_df[final_df['TimeMeasure'].dt.year == 2018]\n",
    "    weather2019 = final_df[final_df['TimeMeasure'].dt.year == 2019]\n",
    "    weather2020 = final_df[final_df['TimeMeasure'].dt.year == 2020]\n",
    "\n",
    "    return weather2018, weather2019, weather2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d38102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the ExtraPoint dataframe\n",
    "#this runs AFTER play and players are preprocessed\n",
    "def make_extraPoint(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the ExtraPoint dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "\n",
    "    '''\n",
    "    play_extrapoint = play_df.loc[play_df['specialTeamsPlayType']=='Extra Point']\n",
    "    #remove extraneous columns\n",
    "    ep = play_extrapoint.drop(columns =['kickReturnYardage', 'kickLength', 'playResult', 'returnerId', 'yardsToGo', 'down', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    ep_play = pd.merge(ep, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    ep_plays=ep_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    ep_plays=ep_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #ep_full = pd.merge(ep_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #eps=ep_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #eps=eps.drop(columns=['nflId'])\n",
    "    return ep_plays\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a870eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make FieldGoal dataframe\n",
    "def make_fieldGoal(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the FieldGoal dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "\n",
    "    '''\n",
    "    play_fieldgoal = play_df.loc[play_df['specialTeamsPlayType']=='Field Goal']\n",
    "    #remove extraneous columns\n",
    "    fg = play_fieldgoal.drop(columns =['kickReturnYardage', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    fg_play = pd.merge(fg, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    fg_plays=fg_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    fg_plays=fg_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #fg_full = pd.merge(fg_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #fgs=fg_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #fgs=fgs.drop(columns=['nflId'])\n",
    "    return fg_plays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8be13",
   "metadata": {},
   "source": [
    "#### Preprocessing functions for actual modeling or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a14ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ep(ep_plays, encode_categorical = True):\n",
    "    '''\n",
    "    This function the ExtraPoint dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "    encode_categorical - Boolean, default is True\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_scale - scaled/processed ExtraPoint dataframe\n",
    "    ep_df - truncated ExtraPoint Dataframe without the scaling.\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode the categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', \n",
    "                   'preSnapVisitorScore', 'kicker_height', 'kicker_weight', #'expected_endzone_y', \n",
    "                   'endzone_y', 'endzone_y_error','endzone_y_off_center']\n",
    "    \n",
    "    columns = ep_plays.columns\n",
    "    \n",
    "    useful_cols.extend(col for col in columns if 'kicker_core_dist' in col)\n",
    "    \n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                 #   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', 'preSnapVisitorScore', \n",
    "                # 'kicker_height', 'kicker_weight', 'blocker_height', 'blocker_weight',\n",
    "                # 'expected_endzone_y', 'endzone_y', 'kicker_core_dist']\n",
    "                \n",
    "    #need to drop nulls for clustering\n",
    "    ep_df = ep_plays[useful_cols].dropna()\n",
    "    \n",
    "    new_eps = ep_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    #new_eps['specialTeamsResult'] = ohe_str\n",
    "    #new_eps['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data, but only non-categorical columns\n",
    "    scale = StandardScaler()\n",
    "    ep_scale = scale.fit_transform(new_eps)\n",
    "    #TO-DO QUESTION: do we want to scale categoricals too? No\n",
    "    \n",
    "    #make this back into a data frame\n",
    "    ep_scale = pd.DataFrame(ep_scale, columns = new_eps.columns)\n",
    "    \n",
    "    #add categorical columns back\n",
    "    if encode_categorical:\n",
    "        #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "        le_str = LabelEncoder()\n",
    "        le_pc = LabelEncoder()\n",
    "        ohe_str = le_str.fit_transform(ep_df['specialTeamsResult'])\n",
    "        ohe_pc = le_pc.fit_transform(ep_df['penaltyCodes'])\n",
    "        \n",
    "        ep_scale['specialTeamsResult'] = ohe_str\n",
    "        ep_scale['penaltyCodes'] = ohe_pc\n",
    "    else:\n",
    "        ep_scale['specialTeamsResult'] = ep_df['specialTeamsResult']\n",
    "        ep_scale['penaltyCodes'] =ep_df['penaltyCodes']\n",
    "        \n",
    "    return ep_scale, ep_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c40fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fg(fg_plays, encode_categorical=True):\n",
    "    '''\n",
    "    This function the FieldGoal dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "    encode_categorical - Boolean, default is True\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_scale - scaled/processed FieldGoal dataframe\n",
    "    fg_df - truncated FieldGoal Dataframe without the scaling.\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "               'gameClockSeconds', 'penaltyCodes', \n",
    "               'penaltyYards', 'preSnapHomeScore', \n",
    "               'preSnapVisitorScore', 'kicker_height', \n",
    "               'kicker_weight', 'down',\n",
    "              'yardsToGo', 'kickLength',\n",
    "              'playResult', #'expected_endzone_y', \n",
    "                   'endzone_y', 'endzone_y_error','endzone_y_off_center']\n",
    "    \n",
    "    columns = fg_plays.columns\n",
    "    \n",
    "    useful_cols.extend(col for col in columns if 'kicker_core_dist' in col)\n",
    "    \n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "   #            'gameClockSeconds', 'penaltyCodes', \n",
    "   #            'penaltyYards', 'preSnapHomeScore', \n",
    "   #            'preSnapVisitorScore', 'kicker_height', \n",
    "   #            'kicker_weight', 'blocker_height', \n",
    "    #           'blocker_weight', 'down',\n",
    "    #          'yardsToGo', 'kickLength',\n",
    "    #          'playResult', 'expected_endzone_y', \n",
    "    #             'endzone_y', 'kicker_core_dist']\n",
    "    \n",
    "    #need to drop nulls for clustering\n",
    "    fg_df = fg_plays[useful_cols].dropna()\n",
    "    \n",
    "    new_fgs = fg_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    #new_fgs['specialTeamsResult'] = ohe_str\n",
    "    #new_fgs['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data, but only non-categorical columns\n",
    "    scale = StandardScaler()\n",
    "    fg_scale = scale.fit_transform(new_fgs)\n",
    "    \n",
    "    #make this back into a data frame\n",
    "    fg_scale = pd.DataFrame(fg_scale, columns = new_fgs.columns)\n",
    "    \n",
    "    #add categorical columns back\n",
    "    if encode_categorical:\n",
    "        #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "        le_str = LabelEncoder()\n",
    "        le_pc = LabelEncoder()\n",
    "        ohe_str = le_str.fit_transform(fg_df['specialTeamsResult'])\n",
    "        ohe_pc = le_pc.fit_transform(fg_df['penaltyCodes'])\n",
    "        \n",
    "        fg_scale['specialTeamsResult'] = ohe_str\n",
    "        fg_scale['penaltyCodes'] = ohe_pc\n",
    "    else:\n",
    "        fg_scale['specialTeamsResult'] = fg_df['specialTeamsResult']\n",
    "        fg_scale['penaltyCodes'] = fg_df['penaltyCodes']\n",
    "    \n",
    "    return fg_scale, fg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd61b90",
   "metadata": {},
   "source": [
    "Let's make the clustering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df_scale, df):\n",
    "    '''\n",
    "    This function performs the clustering on the dataframe df through the scaled data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_scale - the array produced from StandardScaler on the entire preprocessed dataframe df\n",
    "    df - the preprocessed dataframe, prior to encoding\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    df with column 'cluster_id' to track cluster labels\n",
    "    cls - the fit cluster object to make trees, etc.\n",
    "    '''\n",
    "    clusterer = hdbscan.HDBSCAN()\n",
    "    cls = clusterer.fit(df_scale)\n",
    "    df['cluster_id']=cls.labels_\n",
    "    \n",
    "    return cls, df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
