{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb72baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helper Functions :)\n",
    "#convert feet to inches for players\n",
    "def ft_in(x):\n",
    "    if '-' in x:\n",
    "        meas=x.split('-')\n",
    "        #this will be a list ['ft','in']\n",
    "        inches = int(meas[0])*12 + int(meas[1])\n",
    "        return inches\n",
    "    else:\n",
    "        return int(x)\n",
    "    \n",
    "#convert Game Clock from MM:SS:00 to Seconds\n",
    "def clock(x,df):\n",
    "    gameClock = df.loc[x]['gameClock']\n",
    "    quarter = df.loc[x]['quarter']\n",
    "\n",
    "    gameClock_split = gameClock.split(':')\n",
    "\n",
    "    minutes = gameClock_split[0]\n",
    "    seconds = gameClock_split[1]\n",
    "\n",
    "    total_minutes = int(minutes) + 15 * (quarter - 1)\n",
    "\n",
    "    return (total_minutes * 60) + int(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocess by dataset\n",
    "#players Dataset\n",
    "def preprocess_players(players_df):\n",
    "    # preprocessing steps\n",
    "    players_df['height'] = players_df['height'].apply(ft_in)\n",
    "    return players_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d8731",
   "metadata": {},
   "source": [
    "Note: run `preprocess_play` before `preprocess_tracking`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess tracking\n",
    "def preprocess_tracking(track18, track19, track20, play_df):\n",
    "     '''\n",
    "    This function creates the tracking dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    track18, track19, track20 - trackYY.csv dataframes\n",
    "    play_df - Preprocessed play.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    track_ep18 - Tracking ExtraPoint 2018 dataframe\n",
    "    track_ep19 - Tracking ExtraPoint 2019 dataframe\n",
    "    track_ep20 - Tracking ExtraPoint 2020 dataframe\n",
    "    track_fg18 - Tracking FieldGoal 2018 dataframe\n",
    "    track_fg19 - Tracking FieldGoal 2019 dataframe\n",
    "    track_fg20 - Tracking FieldGoal 2020 dataframe\n",
    "    track_punt18 - Tracking Punt 2018 dataframe\n",
    "    track_punt19 - Tracking Punt 2019 dataframe\n",
    "    track_punt20 - Tracking Punt 2020 dataframe\n",
    "    track_ko18 - Tracking Kickoff 2018 dataframe\n",
    "    track_ko19 - Tracking Kickoff 2019 dataframe\n",
    "    track_ko20 - Tracking Kickoff 2020 dataframe\n",
    "    track_fep - Tracking Football ExtraPoint dataframe\n",
    "    track_ffg - Tracking Football Fieldgoal dataframe\n",
    "    track_fpunt - Tracking Football Punt dataframe\n",
    "    track_fko - Tracking Football Kickoff dataframe\n",
    "\n",
    "    '''\n",
    "    #re-orient direction of play by offensive team direction : \n",
    "    #We must reorient this to reflect movement in the offense direction instead of the on-field coordinates \n",
    "    #(reorient the orgin from the bottom left to top right for a change in direction).\n",
    "    #2018 tracking data\n",
    "    track18.loc[track18['playDirection'] == 'left', 'x'] = 120 -track18.loc[track18['playDirection']=='left','x']\n",
    "    track18.loc[track18['playDirection'] == 'left', 'y'] = 160/3 -track18.loc[track18['playDirection']=='left','y']\n",
    "    #note that we have 160/3 for the y direction since the football field is 160ft, but our units are yards\n",
    "\n",
    "    #2019 tracking data\n",
    "    track19.loc[track19['playDirection'] == 'left', 'x'] = 120 -track19.loc[track19['playDirection']=='left','x']\n",
    "    track19.loc[track19['playDirection'] == 'left', 'y'] = 160/3 -track19.loc[track19['playDirection']=='left','y']\n",
    "\n",
    "    #2020 tracking data\n",
    "    track20.loc[track20['playDirection'] == 'left', 'x'] = 120 -track20.loc[track20['playDirection']=='left','x']\n",
    "    track20.loc[track20['playDirection'] == 'left', 'y'] = 160/3 -track20.loc[track20['playDirection']=='left','y']\n",
    "\n",
    "    #divide play dataset by type of play\n",
    "    play_ep = play_df.loc[play_df['specialTeamsPlayType']=='Extra Point'][['gameId', 'playId']]\n",
    "    play_fg = play_df.loc[play_df['specialTeamsPlayType']=='Field Goal'][['gameId', 'playId']]\n",
    "    play_punt = play_df.loc[play_df['specialTeamsPlayType']=='Punt'][['gameId', 'playId']]\n",
    "    play_ko = play_df.loc[play_df['specialTeamsPlayType']=='Kickoff'][['gameId', 'playId']]\n",
    "    \n",
    "    #merge play_type with tracking for each year\n",
    "    #extrapoint\n",
    "    track_ep18 = pd.merge(play_ep, track18, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_ep19 = pd.merge(play_ep, track19, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_ep20 = pd.merge(play_ep, track20, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    \n",
    "    #fieldgoal \n",
    "    track_fg18 = pd.merge(play_fg, track18, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_fg19 = pd.merge(play_fg, track19, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_fg20 = pd.merge(play_fg, track20, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    \n",
    "    #punt\n",
    "    track_punt18 = pd.merge(play_punt, track18, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_punt19 = pd.merge(play_punt, track19, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_punt20 = pd.merge(play_punt, track20, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    \n",
    "    #kickoff\n",
    "    track_ko18 = pd.merge(play_ko, track18, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_ko19 = pd.merge(play_ko, track19, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    track_ko20 = pd.merge(play_ko, track20, left_on = ['gameId', 'playId'], right_on = ['gameId', 'playId'])\n",
    "    \n",
    "    #separate out the football data in each playtype tracking dataframe and drop null values\n",
    "    #concatenate to one dataframe per type of play\n",
    "    #extrapoint football\n",
    "    track_fep18 = track_ep18.loc[track_ep18['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fep19 = track_ep19.loc[track_ep19['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fep20 = track_ep20.loc[track_ep20['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fep = pd.concat([track_fep18, track_fep19, track_fep20], ignore_index = True)\n",
    "    \n",
    "    #fieldgoal football\n",
    "    track_ffg18 = track_fg18.loc[track_fg18['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_ffg19 = track_fg19.loc[track_fg19['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_ffg20 = track_fg20.loc[track_fg20['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_ffg = pd.concat([track_ffg18, track_ffg19, track_ffg20], ignore_index = True)\n",
    "    \n",
    "    #punt football\n",
    "    track_fpunt18 = track_punt18.loc[track_punt18['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fpunt19 = track_punt19.loc[track_punt19['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fpunt20 = track_punt20.loc[track_punt20['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fpunt = pd.concat([track_fpunt18, track_fpunt19, track_fpunt20], ignore_index = True)\n",
    "    \n",
    "    #kickoff football\n",
    "    track_fko18 = track_ko18.loc[track_ko18['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fko19 = track_ko19.loc[track_ko19['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fko20 = track_ko20.loc[track_ko20['displayName'] == 'football'].dropna(axis = 'columns')\n",
    "    track_fko = pd.concat([track_fko18, track_fko19, track_fko20], ignore_index = True)\n",
    "    \n",
    "    return track_ep18, track_ep19, track_ep20, track_fg18, track_fg19, track_fg20, track_punt18, track_punt19, track_punt20, track_ko18, track_ko19, track_ko20, track_fep, track_ffg, track_fpunt, track_fko\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f33705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess play data\n",
    "def preprocess_play(play_df):\n",
    "    #null penalty yards = 0\n",
    "    play_df['penaltyYards']=play_df['penaltyYards'].fillna(0)\n",
    "    \n",
    "    #clock: MM:SS to Seconds\n",
    "    play_df['gameClockSeconds'] = play_df.index.map(lambda x: clock(x,df))\n",
    "    \n",
    "    #redefine nulls in penalty as no penalty\n",
    "    play_df['penaltyCodes']=play_df['penaltyCodes'].fillna('no penalty')\n",
    "    \n",
    "    #TO-DO: Address null values on kickerId and kickBlockerId \n",
    "    #(note their height & weight comes in too)\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424a7d4",
   "metadata": {},
   "source": [
    "#### We need to make some data frames for analysis: Weather, ExtraPoint, FieldGoal, Punts, Kickoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45070894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data():\n",
    "    '''\n",
    "    This function creates the Weather dataframes by year.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    weather2018, weather2019, weather2020 - Weather dataframes by year\n",
    "\n",
    "    '''\n",
    "    # Pull down datasets\n",
    "    games = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/games.csv')\n",
    "    stadium_coordinates = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/stadium_coordinates.csv')\n",
    "    games_weather = pd.read_csv('https://raw.githubusercontent.com/ThompsonJamesBliss/WeatherData/master/data/games_weather.csv')\n",
    "\n",
    "    # Merge game and weather data on game_id\n",
    "    games_weather_merge = pd.merge(games_weather, games, on='game_id')\n",
    "\n",
    "    # Merge stadium data on StadiumName\n",
    "    final_df = pd.merge(games_weather_merge, stadium_coordinates, on='StadiumName')\n",
    "\n",
    "    # Convert time columns to datetime objects\n",
    "    time_cols = ['TimeMeasure', 'TimeStartGame', 'TimeEndGame']\n",
    "\n",
    "    for col in time_cols:\n",
    "        final_df[col] = pd.to_datetime(final_df[col], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "    # Create sliced DataFrames\n",
    "    weather2018 = final_df[final_df['TimeMeasure'].dt.year == 2018]\n",
    "    weather2019 = final_df[final_df['TimeMeasure'].dt.year == 2019]\n",
    "    weather2020 = final_df[final_df['TimeMeasure'].dt.year == 2020]\n",
    "\n",
    "    return weather2018, weather2019, weather2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74360896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the ExtraPoint dataframe\n",
    "#this runs AFTER play and players are preprocessed\n",
    "def make_extraPoint(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the ExtraPoint dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "\n",
    "    '''\n",
    "    play_extrapoint = play_df.loc[play_df['specialTeamsPlayType']=='Extra Point']\n",
    "    #remove extraneous columns\n",
    "    ep = play_extrapoint.drop(columns =['kickReturnYardage', 'kickLength', 'playResult', 'returnerId', 'yardsToGo', 'down', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    ep_play = pd.merge(ep, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    ep_plays=ep_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    ep_plays=ep_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #ep_full = pd.merge(ep_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #eps=ep_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #eps=eps.drop(columns=['nflId'])\n",
    "    return ep_plays\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make FieldGoal dataframe\n",
    "def make_fieldGoal(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the FieldGoal dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "\n",
    "    '''\n",
    "    play_fieldgoal = play_df.loc[play_df['specialTeamsPlayType']=='Field Goal']\n",
    "    #remove extraneous columns\n",
    "    fg = play_fieldgoal.drop(columns =['kickReturnYardage', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    fg_play = pd.merge(fg, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    fg_plays=fg_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    fg_plays=fg_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #fg_full = pd.merge(fg_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #fgs=fg_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #fgs=fgs.drop(columns=['nflId'])\n",
    "    return fg_plays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbb71a",
   "metadata": {},
   "source": [
    "#### Preprocessing functions for actual modeling or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f37883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ep(ep_plays):\n",
    "    '''\n",
    "    This function the ExtraPoint dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_scale - processed ExtraPoint dataframe\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode the categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', \n",
    "                   'preSnapVisitorScore', 'kicker_height', 'kicker_weight']\n",
    "    \n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                 #   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', 'preSnapVisitorScore', \n",
    "                # 'kicker_height', 'kicker_weight', 'blocker_height', 'blocker_weight']\n",
    "    ep_df = ep_plays[useful_cols]\n",
    "    #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "    le_str = LabelEncoder()\n",
    "    le_pc = LabelEncoder()\n",
    "    ohe_str = le_str.fit_transform(ep_df['specialTeamsResult'])\n",
    "    ohe_pc = le_pc.fit_transform(ep_df['penaltyCodes'])\n",
    "    new_eps = ep_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    #new_eps['specialTeamsResult'] = ohe_str\n",
    "    #new_eps['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data, but only non-categorical columns\n",
    "    scale = StandardScaler()\n",
    "    ep_scale = scale.fit_transform(new_eps)\n",
    "    #TO-DO QUESTION: do we want to scale categoricals too? No\n",
    "    \n",
    "    #add categorical columns back\n",
    "    ep_scale['specialTeamsResult'] = ohe_str\n",
    "    ep_scale['penaltyCodes'] = ohe_pc\n",
    "    #we are running a distance dependent algorithm\n",
    "    return ep_scale\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fg(fg_plays):\n",
    "    '''\n",
    "    This function the FieldGoal dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_scale - processed FieldGoal dataframe\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "               'gameClockSeconds', 'penaltyCodes', \n",
    "               'penaltyYards', 'preSnapHomeScore', \n",
    "               'preSnapVisitorScore', 'kicker_height', \n",
    "               'kicker_weight', 'down',\n",
    "              'yardsToGo', 'kickLength',\n",
    "              'playResult']\n",
    "    \n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "   #            'gameClockSeconds', 'penaltyCodes', \n",
    "   #            'penaltyYards', 'preSnapHomeScore', \n",
    "   #            'preSnapVisitorScore', 'kicker_height', \n",
    "   #            'kicker_weight', 'blocker_height', \n",
    "    #           'blocker_weight', 'down',\n",
    "    #          'yardsToGo', 'kickLength',\n",
    "    #          'playResult']\n",
    "    fg_df = fg_plays[useful_cols]\n",
    "    #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "    le_str = LabelEncoder()\n",
    "    le_pc = LabelEncoder()\n",
    "    ohe_str = le_str.fit_transform(fg_df['specialTeamsResult'])\n",
    "    ohe_pc = le_pc.fit_transform(fg_df['penaltyCodes'])\n",
    "    new_fgs = fg_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    #new_fgs['specialTeamsResult'] = ohe_str\n",
    "    #new_fgs['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data, but only non-categorical columns\n",
    "    scale = StandardScaler()\n",
    "    fg_scale = scale.fit_transform(new_fgs)\n",
    "    #TO-DO QUESTION: do we want to scale categoricals too? No\n",
    "    \n",
    "    #add categorical columns back\n",
    "    fg_scale['specialTeamsResult'] = ohe_str\n",
    "    fg_scale['penaltyCodes'] = ohe_pc\n",
    "    #we are running a distance dependent algorithm\n",
    "    return fg_scale, fg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe297db2",
   "metadata": {},
   "source": [
    "Let's make the clustering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce249b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df_scale, df):\n",
    "    '''\n",
    "    This function performs the clustering on the dataframe df through the scaled data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_scale - the array produced from StandardScaler on the entire preprocessed dataframe df\n",
    "    df - the preprocessed dataframe, prior to encoding\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    df with column 'cluster_id' to track cluster labels\n",
    "    cls - the fit cluster object to make trees, etc.\n",
    "    '''\n",
    "    clusterer = hdbscan.HDBSCAN()\n",
    "    cls = clusterer.fit(df_scale)\n",
    "    df['cluster_id']=cls.labels_\n",
    "    \n",
    "    return cls, df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
