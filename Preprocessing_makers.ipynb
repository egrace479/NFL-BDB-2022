{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6ba9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Helper Functions :)\n",
    "#convert feet to inches for players\n",
    "def ft_in(x):\n",
    "    if '-' in x:\n",
    "        meas=x.split('-')\n",
    "        #this will be a list ['ft','in']\n",
    "        inches = int(meas[0])*12 + int(meas[1])\n",
    "        return inches\n",
    "    else:\n",
    "        return int(x)\n",
    "    \n",
    "#convert Game Clock from MM:SS:00 to Seconds\n",
    "def clock(x):\n",
    "    gameClock = eps.loc[x]['gameClock']\n",
    "    quarter = eps.loc[x]['quarter']\n",
    "\n",
    "    gameClock_split = gameClock.split(':')\n",
    "\n",
    "    minutes = gameClock_split[0]\n",
    "    seconds = gameClock_split[1]\n",
    "\n",
    "    total_minutes = int(minutes) + 15 * (quarter - 1)\n",
    "\n",
    "    return (total_minutes * 60) + int(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f645c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##preprocess by dataset\n",
    "#players Dataset\n",
    "def preprocess_players(players_df):\n",
    "    # preprocessing steps\n",
    "    players_df['height'] = players_df['height'].apply(ft_in)\n",
    "    return players_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21249100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess tracking\n",
    "def preprocess_tracking(track_df):\n",
    "    track_df.loc[track_df['playDirection'] == 'left', 'x'] = 120 -track_df.loc[track_df['playDirection']=='left','x']\n",
    "    track_df.loc[track_df['playDirection'] == 'left', 'y'] = 160/3 -track_df.loc[track_df['playDirection']=='left','y']\n",
    "#note that we have 160/3 for the y direction since the football field is 160ft, but our units are yards\n",
    "    return track_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess play data\n",
    "def preprocess_play(play_df):\n",
    "    #null penalty yards = 0\n",
    "    play_df['penaltyYards']=play_df['penaltyYards'].fillna(0)\n",
    "    \n",
    "    #clock: MM:SS to Seconds\n",
    "    play_df['gameClockSeconds'] = play_df.index.map(lambda x: clock(x))\n",
    "    \n",
    "    #redefine nulls in penalty as no penalty\n",
    "    play_df['penaltyCodes']=play_df['penaltyCodes'].fillna('no penalty')\n",
    "    \n",
    "    #TO-DO: Address null values on kickerId and kickBlockerId \n",
    "    #(note their height & weight comes in too)\n",
    "    return play_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68299adb",
   "metadata": {},
   "source": [
    "#### We need to make some data frames for analysis: Weather, ExtraPoint, FieldGoal, Punts, Kickoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data():\n",
    "    '''\n",
    "    This function creates the Weather dataframes by year.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    weather2018, weather2019, weather2020 - Weather dataframes by year\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Merge game and weather data on game_id\n",
    "    g_weather_merge = pd.merge(g_weather, game_ident, on='game_id')\n",
    "\n",
    "    # Merge stadium data on StadiumName\n",
    "    final_df = pd.merge(g_weather_merge, stadium_ident, on='StadiumName')\n",
    "\n",
    "    # Convert time columns to datetime objects\n",
    "    time_cols = ['TimeMeasure', 'TimeStartGame', 'TimeEndGame']\n",
    "\n",
    "    for col in time_cols:\n",
    "        final_df[col] = pd.to_datetime(final_df[col], format='%m/%d/%Y %H:%M')\n",
    "\n",
    "    # Create sliced DataFrames\n",
    "    weather2018 = final_df[final_df['TimeMeasure'].dt.year == 2018]\n",
    "    weather2019 = final_df[final_df['TimeMeasure'].dt.year == 2019]\n",
    "    weather2020 = final_df[final_df['TimeMeasure'].dt.year == 2020]\n",
    "\n",
    "    return weather2018, weather2019, weather2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65a0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the ExtraPoint dataframe\n",
    "#this runs AFTER play and players are preprocessed\n",
    "def make_extraPoint(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the ExtraPoint dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "\n",
    "    '''\n",
    "    play_extrapoint = play_df.loc[play_df['specialTeamsPlayType']=='Extra Point']\n",
    "    #remove extraneous columns\n",
    "    ep = play_extrapoint.drop(columns =['kickReturnYardage', 'kickLength', 'playResult', 'returnerId', 'yardsToGo', 'down', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    ep_play = pd.merge(ep, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    ep_plays=ep_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    ep_plays=ep_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #ep_full = pd.merge(ep_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #eps=ep_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #eps=eps.drop(columns=['nflId'])\n",
    "    return ep_plays\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make FieldGoal dataframe\n",
    "def make_fieldGoal(play_df, players_df):\n",
    "    '''\n",
    "    This function creates the FieldGoal dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    play_df - Preprocessed players.csv dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "\n",
    "    '''\n",
    "    play_fieldgoal = play_df.loc[play_df['specialTeamsPlayType']=='Field Goal']\n",
    "    #remove extraneous columns\n",
    "    fg = play_fieldgoal.drop(columns =['kickReturnYardage', 'specialTeamsPlayType'])\n",
    "   \n",
    "    #add in Kickers\n",
    "    fg_play = pd.merge(fg, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "             left_on = 'kickerId', right_on = 'nflId')\n",
    "    fg_plays=fg_play.rename(columns = {\"height\": 'kicker_height', \"weight\": 'kicker_weight', \"Position\": 'kicker_position', \"displayName\": 'kicker_name'})\n",
    "    fg_plays=fg_plays.drop(columns=['nflId'])\n",
    "    #add in Blockers (figure out Nulls first!)\n",
    "    #fg_full = pd.merge(fg_plays, players_df[['nflId', 'height', 'weight','Position', 'displayName']], how = 'left',\n",
    "    #         left_on = 'kickBlockerId', right_on = 'nflId')\n",
    "    #fgs=fg_full.rename(columns = {\"height\": 'blocker_height', \"weight\": 'blocker_weight', \"Position\": 'blocker_position', \"displayName\": 'blocker_name'})\n",
    "    #fgs=fgs.drop(columns=['nflId'])\n",
    "    return fg_plays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1447d",
   "metadata": {},
   "source": [
    "#### Preprocessing functions for actual modeling or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ep(ep_plays):\n",
    "    '''\n",
    "    This function the ExtraPoint dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ep_plays - ExtraPoint dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    ep_scale - processed ExtraPoint dataframe\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode the categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', \n",
    "                   'preSnapVisitorScore', 'kicker_height', 'kicker_weight']\n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', 'gameClockSeconds', \n",
    "                 #   'penaltyCodes', 'penaltyYards', 'preSnapHomeScore', 'preSnapVisitorScore', \n",
    "                # 'kicker_height', 'kicker_weight', 'blocker_height', 'blocker_weight']\n",
    "    ep_df = ep_plays[useful_cols]\n",
    "    #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "    le_str = LabelEncoder()\n",
    "    le_pc = LabelEncoder()\n",
    "    ohe_str = le_str.fit_transform(ep_df['specialTeamsResult'])\n",
    "    ohe_pc = le_pc.fit_transform(ep_df['penaltyCodes'])\n",
    "    new_eps = ep_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    new_eps['specialTeamsResult'] = ohe_str\n",
    "    new_eps['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data\n",
    "    scale = StandardScaler()\n",
    "    ep_scale = scale.fit_transform(new_eps)\n",
    "    #TO-DO QUESTION: do we want to scale categoricals too? \n",
    "    #we are running a distance dependent algorithm\n",
    "    return ep_scale\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fg(fg_plays):\n",
    "    '''\n",
    "    This function the FieldGoal dataframe for clustering.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fg_plays - FieldGoal dataframe\n",
    "    ...\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    fg_scale - processed FieldGoal dataframe\n",
    "\n",
    "    '''\n",
    "    #reduce number of columns to those with numeric values or one-hot-encode categoricals\n",
    "    useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "               'gameClockSeconds', 'penaltyCodes', \n",
    "               'penaltyYards', 'preSnapHomeScore', \n",
    "               'preSnapVisitorScore', 'kicker_height', \n",
    "               'kicker_weight', 'down',\n",
    "              'yardsToGo', 'kickLength',\n",
    "              'playResult']\n",
    "    #useful_cols with blockers\n",
    "    #useful_cols = ['specialTeamsResult', 'yardlineNumber', \n",
    "   #            'gameClockSeconds', 'penaltyCodes', \n",
    "   #            'penaltyYards', 'preSnapHomeScore', \n",
    "   #            'preSnapVisitorScore', 'kicker_height', \n",
    "   #            'kicker_weight', 'blocker_height', \n",
    "    #           'blocker_weight', 'down',\n",
    "    #          'yardsToGo', 'kickLength',\n",
    "    #          'playResult']\n",
    "    fg_df = fg_plays[useful_cols]\n",
    "    #one-hot-encode SpecialTeamsResult and penaltyCodes\n",
    "    le_str = LabelEncoder()\n",
    "    le_pc = LabelEncoder()\n",
    "    ohe_str = le_str.fit_transform(fg_df['specialTeamsResult'])\n",
    "    ohe_pc = le_pc.fit_transform(fg_df['penaltyCodes'])\n",
    "    new_fgs = fg_df.drop(['specialTeamsResult', 'penaltyCodes'], axis=1)\n",
    "    new_fgs['specialTeamsResult'] = ohe_str\n",
    "    new_fgs['penaltyCodes'] = ohe_pc\n",
    "    \n",
    "    #scale data\n",
    "    scale = StandardScaler()\n",
    "    fg_scale = scale.fit_transform(new_fgs)\n",
    "    #TO-DO QUESTION: do we want to scale categoricals too? \n",
    "    #we are running a distance dependent algorithm\n",
    "    return fg_scale, fg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e770f67",
   "metadata": {},
   "source": [
    "Let's make the clustering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(df_scale, df):\n",
    "    '''\n",
    "    This function performs the clustering on the dataframe df through the scaled data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_scale - the array produced from StandardScaler on the entire preprocessed dataframe df\n",
    "    df - the preprocessed dataframe, prior to encoding\n",
    "    \n",
    "    Returns:\n",
    "    ---------\n",
    "    df with column 'cluster_id' to track cluster labels\n",
    "    cls - the fit cluster object to make trees, etc.\n",
    "    '''\n",
    "    clusterer = hdbscan.HDBSCAN()\n",
    "    cls = clusterer.fit(df_scale)\n",
    "    df['cluster_id']=cls.labels_\n",
    "    \n",
    "    return cls, df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
